<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on 谢 XIE</title>
    <link>/posts/</link>
    <description>Recent content in Blog on 谢 XIE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 26 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>data.table 与 pandas</title>
      <link>/posts/2019-12-26-dt-pd/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-12-26-dt-pd/</guid>
      <description>数据分析项目通常可以分解为以下过程，数据加载-数据清洗-(特征处理、可视化、模型训练)-成果汇报1。其中，数据清洗与特征处理或者称为数据预处理过程，一般会占据整个项目的大部分时间。熟练掌握相关工具，提高数据处理的效率，是开展数据分析工作的基础。 在开展数据科学相关工作时，最常用的开源工具包括 R 与 python。对于可在内存级处理的数据，在 R 中通常使用 data.table 包进行数据处理，而在 python 环境中 pandas 包最为常用的。为了方便查阅和对比，本文分别用 data.table 与 pandas 实现了常见的数据处理任务2。
数据框（data frame）是大家接触最多的数据格式，它的每一列都是长度相等、类型一致的向量。对数据框的操作可以从行与列两个维度，拆解为以下五类基本操作。这一思路来自 dplyr 包3的帮助文档，因此下面五类基本操作的英文均为该包的函数名。这些基本操作均可以与 group_by 相互结合使用。除了这五类基本操作，还包括行列转换、数据框的切割与合并等。绝多数的数据处理任务都可以拆解为以上这几类基本操作，具体案例请参见下面的代码。
行：选择 filter、排序 arrange 列：选择 select、新建 mutate、计算 summarise 数据探索 数据加载 library(data.table) packageVersion(&amp;#39;data.table&amp;#39;) url = &amp;quot;https://vincentarelbundock.github.io/Rdatasets/csv/datasets/HairEyeColor.csv&amp;quot; dt = fread(url) import pandas as pd pd.__version__ url = &amp;quot;https://vincentarelbundock.github.io/Rdatasets/csv/datasets/HairEyeColor.csv&amp;quot; df = pd.read_csv(url) 查看数据结构 # 数据类型 class(dt) str(dt) # 列名 names(dt) # 打印前后几行 head(dt, n=3) tail(dt, n=3) # 维度 dim(dt) nrow(dt) ncol(dt) # 统计描述 summary(dt) # 数据类型 type(df) df.</description>
    </item>
    
    <item>
      <title>互联网金融的大数据风控[转]</title>
      <link>/posts/2018-01-10-fintech-credit/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/2018-01-10-fintech-credit/</guid>
      <description>大数据能够进行数据变现的商业模式目前主要包括，一是精准营销，典型的场景是商品推荐和精准广告投放，二是大数据风控，典型的场景是互联网金融的大数据风控。^[来源: 知乎]
金融的本质是风险管理，风控是所有金融业务的核心。典型的金融借贷业务例如抵押贷款、消费贷款、供应链金融、以及票据融资都需要数据风控识别欺诈用户及评估用户信用等级。
传统金融的风控主要利用了信用属性强大的金融数据，一般采用20个纬度左右的数据，利用评分来识别客户的还款能力和还款意愿。信用相关程度强的数据纬度为十个左右，包含年龄、职业、收入、学历、工作单位、借贷情况、房产，汽车、单位、还贷记录等，金融企业参考用户提交的数据进行打分，最后得到申请人的信用评分，依据评分来决定是否贷款以及贷款额度。其他同信用相关的数据还有区域、产品、理财方式、行业、缴款方式、缴款记录、金额、时间、频率等。
互联网金融的大数据风控并不是完全改变传统风控，实际是丰富传统风控的数据纬度。互联网风控中，首先还是利用信用属性强的金融数据，判断借款人的还款能力和还款意愿，然后在利用信用属性较弱的行为数据进行补充，一般是利用数据的关联分析来判断借款人的信用情况，借助数据模型来揭示某些行为特征和信用风险之间的关系。
互联网金融公司利用大数据进行风控时，都是利用多维度数据来识别借款人风险。同信用相关的数据越多地被用于借款人风险评估，借款人的信用风险就被揭示的更充分，信用评分就会更加客观，接近借款人实际风险。常用的互联网金融大数据风控方式有以下几种：
欺诈识别: 身份验证、线上申请信息与行为、黑名单与灰名单、移动设备信息 信用风险: 消费记录、社会关系、社会属性与行为、司法信息 一、验证借款人身份 验证借款人身份的五因素认证是姓名、手机号、身份证号、银行卡号、家庭地址。企业可以借助国政通的数据来验证姓名、身份证号，借助银联数据来验证银行卡号和姓名，利用运营商数据来验证手机号、姓名、身份证号、家庭住址。
如果借款人是欺诈用户，这五个信息都可以买到。这个时候就需要进行人脸识别了，人脸识别等原理是调用国政通/公安局API接口，将申请人实时拍摄的照片/视频同客户预留在公安的身份证进行识别，通过人脸识别技术验证申请人是否是借款人本人。
其他的验证客户的方式包括让客户出示其他银行的信用卡及刷卡记录，或者验证客户的学历证书和身份认证。
二、分析提交的信息来识别欺诈 大部分的贷款申请都从线下移到了线上，特别是在互联网金融领域，消费贷和学生贷都是以线上申请为主的。
线上申请时，申请人会按照贷款公司的要求填写多维度信息例如户籍地址，居住地址，工作单位，单位电话，单位名称等。如果是欺诈用户，其填写的信息往往会出现一些规律，企业可根据异常填写记录来识别欺诈。例如填写不同城市居住小区名字相同、填写的不同城市，不同单位的电话相同、不同单位的地址街道相同、单位名称相同、甚至居住的楼层和号码都相同。还有一些填写假的小区、地址和单位名称以及电话等。
如果企业发现一些重复的信息和电话号码，申请人欺诈的可能性就会很高。
三、分析客户线上申请行为来识别欺诈 欺诈用户往往事先准备好用户基本信息，在申请过程中，快速进行填写，批量作业，在多家网站进行申请，通过提高申请量来获得更多的贷款。
企业可以借助于SDK或JS来采集申请人在各个环节的行为，计算客户阅读条款的时间，填写信息的时间，申请贷款的时间等，如果这些申请时间大大小于正常客户申请时间，例如填写地址信息小于2秒，阅读条款少于3秒钟，申请贷款低于20秒等。用户申请的时间也很关键，一般晚上11点以后申请贷款的申请人，欺诈比例和违约比例较高。
这些异常申请行为可能揭示申请人具有欺诈倾向，企业可以结合其他的信息来判断客户是否为欺诈用户。
四、利用黑名单和灰名单识别风险 互联网金融公司面临的主要风险为恶意欺诈，70%左右的信贷损失来源于申请人的恶意欺诈。客户逾期或者违约贷款中至少有30%左右可以收回，另外的一些可以通过催收公司进行催收，M2逾期的回收率在20%左右。
市场上有近百家的公司从事个人征信相关工作，其主要的商业模式是反欺诈识别，灰名单识别，以及客户征信评分。反欺诈识别中，重要的一个参考就是黑名单，市场上领先的大数据风控公司拥有将近1000万左右的黑名单，大部分黑名单是过去十多年积累下来的老赖名单，真正有价值的黑名单在两百万左右。
黑名单来源于民间借贷、线上P2P、信用卡公司、小额借贷等公司的历史违约用户，其中很大一部分不再有借贷行为，参考价值有限。另外一个主要来源是催收公司，催收的成功率一般小于于30%(M3以上的)，会产生很多黑名单。灰名单是逾期但是还没有达到违约的客户(逾期少于3个月的客户)，灰名单也还意味着多头借贷，申请人在多个贷款平台进行借贷。总借款数目远远超过其还款能力。
黑名单和灰名单是很好的风控方式，但是各个征信公司所拥有的名单仅仅是市场总量的一部分，很多互联网金融公司不得不接入多个风控公司，来获得更多的黑名单来提高查得率。央行和上海经信委正在联合多家互联网金融公司建立统一的黑名单平台，但是很多互联网金融公司都不太愿意贡献自家的黑名单，这些黑名单是用真金白银换来的教训。另外如果让外界知道了自家平台黑名单的数量，会影响其公司声誉，降低公司估值，并令投资者质疑其平台的风控水平。
五、利用移动设备数据识别欺诈行为 数据中一个比较特殊的就是移动设备数据反欺诈，公司可以利用移动设备的位置信息来验证客户提交的工作地和生活地是否真实，另外来可以根据设备安装的应用活跃来识别多头借贷风险。
欺诈用户一般会使用模拟器进行贷款申请，移动大数据可以识别出贷款人是否使用模拟器。欺诈用户也有一些典型特征，例如很多设备聚集在一个区域，一起申请贷款。欺诈设备不安装生活和工具用App，仅仅安装和贷款有关的App，可能还安装了一些密码破译软件或者其他的恶意软件。
欺诈用户还有可能不停更换SIM卡和手机，利用SIM卡和手机绑定时间和频次可以识别出部分欺诈用户。另外欺诈用户也会购买一些已经淘汰的手机，其机器上面的操作系统已经过时很久，所安装的App版本都很旧。这些特征可以识别出一些欺诈用户。
六、利用消费记录来进行评分 大数据风控除了可以识别出坏人，还可以评估贷款人的还款能力。过去传统金融依据借款人的收入来判断其还款能力，但是有些客户拥有工资以外的收入，例如投资收入、顾问咨询收入等。另外一些客户可能从父母、伴侣、朋友那里获得其他的财政支持，拥有较高的支付能力。
按照传统金融的做法，在家不工作照顾家庭的主妇可能还款能力较弱。无法给其提供贷款，但是其丈夫收入很高，家庭日常支出由其太太做主。这种情况，就需要消费数据来证明其还款能力了。
常用的消费记录由银行卡消费、电商购物、公共事业费记录、大宗商品消费等。还可以参考航空记录、手机话费、特殊会员消费等方式。例如头等舱乘坐次数，物业费高低、高尔夫球俱乐部消费，游艇俱乐部会员费用，奢侈品会员，豪车4S店消费记录等消费数据可以作为其信用评分重要参考。
互联网金融的主要客户是屌丝，其电商消费记录、旅游消费记录、以及加油消费记录都可以作为评估其信用的依据。有的互联金融公司专门从事个人电商消费数据分析，只要客户授权其登陆电商网站，其可以借助于工具将客户历史消费数据全部抓取并进行汇总和评分。
七、参考社会关系来评估信用情况 物以类聚，人与群分。一般情况下，信用好的人，他的朋友信用也很好。信用不好的人，他的朋友的信用分也很低。
参考借款人常联系的朋友信用评分可以评价借款人的信用情况，一般会采用经常打电话的朋友作为样本，评估经常联系的几个人(不超过6六个人)的信用评分，去掉一个最高分，去掉一个最低分，取其中的平均值来判断借款人的信用。这种方式挑战很大，只是依靠手机号码来判断个人信用可信度不高。一般仅仅用于反欺诈识别，利用其经常通话的手机号在黑名单库里面进行匹配，如果命中，则此申请人的风险较高，需要进一步进行调查。
八、参考借款人社会属性和行为来评估信用 参考过去互联网金融风控的经验发现，拥有伴侣和子女的借款人，其贷款违约率较低;年龄大的人比年龄低的人贷款违约率要高，其中50岁左右的贷款人违约率最高，30岁左右的人违约率最低。贷款用于家庭消费和教育的贷款人，其贷款违约率低;声明月收入超过3万的人比声明月收入低于1万5千的人贷款违约率高;贷款次数多的人，其贷款违约率低于第一次贷款的人。
经常不交公共事业费和物业费的人，其贷款违约率较高。经常换工作，收入不稳定的人贷款违约率较高。经常参加社会公益活动的人，成为各种组织会员的人，其贷款违约率低。经常更换手机号码的人贷款违约率比一直使用一个电话号码的人高很多。
午夜经常上网，很晚发微博，生活不规律，经常在各个城市跑的申请人，其带贷款违约率比其他人高30%。刻意隐瞒自己过去经历和联系方式，填写简单信息的人，比信息填写丰富的人违约概率高20%。借款时间长的人比借款时间短短人，逾期和违约概率高20%左右。拥有汽车的贷款人比没有汽车的贷款人，贷款违约率低10%左右。
九、利用司法信息评估风险 涉毒涉赌以及涉嫌治安处罚的人，其信用情况不是太好，特别是涉赌和涉毒人员，这些人是高风险人群，一旦获得贷款，其贷款用途不可控，贷款有可能不会得到偿还。
寻找这些涉毒涉赌的嫌疑人，可以利用当地的公安数据，但是难度较大。也可以采用移动设备的位置信息来进行一定程度的识别。如果设备经常在半夜出现在赌博场所或赌博区域例如澳门，其申请人涉赌的风险就较高。另外中国有些特定的地区，当地的有一部分人群从事涉赌或涉赌行业，一旦申请人填写的居住地址或者移动设备位置信息涉及这些区域，也要引起重视。涉赌和涉毒的人员工作一般也不太稳定或者没有固定工作收入，如果申请人经常换工作或者经常在某一个阶段没有收入，这种情况需要引起重视。涉赌和涉毒的人活动规律比较特殊，经常半夜在外面活动，另外也经常住本地宾馆，这些信息都可以参考移动大数据进行识别。
总之，互联网金融的大数据风控采用了用户社会行为和社会属性数据，在一定程度上补充了传统风控数据维度不足的缺点，能够更加全面识别出欺诈客户，评价客户的风险水平。互联网金融企业通过分析申请人的社会行为数据来控制信用风险，将资金借给合格贷款人，保证资金的安全。</description>
    </item>
    
    <item>
      <title>使用 R 语言开发评分卡模型</title>
      <link>/posts/2018-01-05-scorecard/</link>
      <pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/2018-01-05-scorecard/</guid>
      <description>为了提高评分卡模型的开发效率，我为 R 语言社区贡献了一个开源项目 scorecard 包 (HomePage, Github, CRAN)。该 R 包提供了评分卡开发过程中的常用功能，包括变量粗筛、分箱与 woe 转换、模型评估、评分刻度转换等。
评分卡模型的开发流程通常包括以下五个主要步骤：数据准备、WOE 分箱、模型拟合、模型评估、评分卡刻度。下面结合 scorecard 包完成一个简单的评分卡模型开发案例。更加详细的评分卡模型开发介绍请参考幻灯片。
数据准备 首先加载 scorecard 包，并载入包内自带的德国信贷数据集。该数据集包含了1000个借款人的信贷数据，20个 X 特征与1个 Y 值。其详细信息参见 UCI 的德国信贷数据集网站。
library(scorecard) # load germancredit data data(germancredit) 载入数据集后，可先通过变量的 IV 值、缺失率以及单类别率对 X 特征进行初步筛选。var_filter 函数默认删除信息值小于0.02、缺失率大于95%或单类别比例大于95%的变量。var_filter 函数还能够人为设定需要删除或保留的变量，以及够返回变量删除的原因列表。
# filter variable via missing rate, iv, identical rate dt = var_filter(germancredit, y = &amp;#39;creditability&amp;#39;) ## ✔ 1 variables are removed via identical_rate ## ✔ 6 variables are removed via info_value ## ✔ Variable filtering on 1000 rows and 20 columns in 00:00:00 ## ✔ 7 variables are removed in total 将经过初筛的数据集拆分为训练集与测试集。在 split_df 函数中如果指定了 y 变量，那么将基于 y 变量分层拆分，如果没有指定，则随机拆分数据集。ratio 为拆分后两个数据集的样本量占比。 seed 为随机种子，用于重现拆分的样本。</description>
    </item>
    
    <item>
      <title>stringr 与 regex 函数对应关系</title>
      <link>/posts/2017-07-25-stringr-regex/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/2017-07-25-stringr-regex/</guid>
      <description>stringr 是 Hadley 大神写的 tidyverse1 系列数据处理包中专门用于处理文本数据的，其函数命名统一易于记忆。而 R 基础包中的文本处理函数 (regex) 的命名规则不是特别统一。下表给出了主要函数之间的映射关系，便于以后查看2。
stringr包中函数 功能说明 R Base 中对应函数 使用正则表达式的函数 str_extract() 提取首个匹配模式的字符 regmatches() str_extract_all() 提取所有匹配模式的字符 regmatches() str_locate() 返回首个匹配模式的字符的位置 regexpr() str_locate_all() 返回所有匹配模式的字符的位置 gregexpr() str_replace() 替换首个匹配模式 sub() str_replace_all() 替换所有匹配模式 gsub() str_split() 按照模式分割字符串 strsplit() str_split_fixed() 按照模式将字符串分割成指定个数 - str_detect() 检测字符是否存在某些指定模式 grepl() str_count() 返回指定模式出现的次数 - 其他重要函数 str_sub() 提取指定位置的字符 regmatches() str_dup() 丢弃指定位置的字符 - str_length() 返回字符的长度 nchar() str_pad() 填补字符 - str_trim() 丢弃填充，如去掉字符前后的空格 - str_c() 连接字符 paste(),paste0() r 与 python 中三个数据处理系列包： data.table、 tidyverse、 pandas&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>Markdown相关资料</title>
      <link>/posts/2017-04-05-markdown/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/2017-04-05-markdown/</guid>
      <description> John Gruber&amp;rsquo;s Markdown syntax ( 中文翻译) John MacFarlane&amp;rsquo;s Pandoc Markdown ( 中文翻译) Blackfriday Markdown ( github) RMarkdown ( github) MathJax ( 中文版) </description>
    </item>
    
    <item>
      <title>博客搭建过程</title>
      <link>/posts/2017-03-21-new-site/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/2017-03-21-new-site/</guid>
      <description>在“我网故我在”的召唤下，我使用 R 语言的 blogdown 包1 和 GitHubPages 在一小时内搭建了本博客。搭建过程分为三个步骤：编辑网站文件、创建 GitHub Pages 仓库、域名绑定。
编辑网站文件 首先需要编写网站文件，也就是一堆 HTML、JS、CSS 文件。 益辉的 blogdown 让静态网站文件编写简单到了一条 R 语句。在编辑网站文件之前，最好用 RStudio 新建一个空的项目文件夹，便于文件管理。在 R 中敲入如下代码
# 安装blogdown包 devtools::install_github(&amp;#39;rstudio/blogdown&amp;#39;) setwd(path) # path为新建的项目文件夹路径 # 或者直接用rstudio打开*.Rproj文件 # 创建网站文件 blogdown::new_site() # 默认主题 theme = &amp;#34;yihui/hugo-lithium&amp;#34; # &amp;gt; sessionInfo() # R语言系统环境 # R version 3.3.2 (2016-10-31) # Platform: x86_64-apple-darwin13.4.0 (64-bit) # Running under: macOS Sierra 10.12.3 创建GitHub Pages仓库 登录自己的 github 主页（例如我的主页 https://github.com/shichenxie，其中shichenxie为我的 github 账号），新建名为shichenxie.github.io的项目仓库 (repository)。
然后将 blogdown 创建的 public 文件夹上传到 github pages 文件夹中。在 terminal 中敲入如下代码2</description>
    </item>
    
  </channel>
</rss>
